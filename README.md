# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.


# Summary

## The Problem Statement:
This dataset contains data about a bank direct marketing campaigns; where the objective [for decision intel foresights] is on product sales (if a term-deposit could be subscribed to, when a bank client is contacted?).
[... more on the dataset whitepaper](https://core.ac.uk/download/pdf/55631291.pdf)

## The Solution:
The best performing model was an Azure AutoML algorithm of "VotingEnsemble" that scored a %91.69 overall accuracy [with Stopping criteria reached at iteration 32]; in contrast to the hyperdrive run that scored a lower accuracy of %91.25.


# Scikit-learn Pipeline

## The Pipeline Architecture Explanation:

A compute CPU cluster was created with Target_Cluster/AmlCompute objects, while TabularDataset was used to import (csv) data from a web link (path/URL) using Azure SDK.

The pipeline steps begin with direct downloading of an online csv file's URL to be ingested into an TabularDataset object, the dataset factory before data get cleaned with the training script clean_data function, before the converted dataframe lables get serialised then split into 70% for training and 30% for testing, before it gets classified with HyperDrive hyperparameters tuning and regularization (with uniform random sampling) via a Logistic Regression algorithm, for the best berforming model to then be registerd.

Inintial Data Prep was performed with one_hot_encode on columns so that their Dataflow can have (for each categorical label) new binary columns.

As for the data architecture, then notwithstanding that Dataset.from_delimited_files() was used instead of [Dataset.from_pandas_dataframe()](https://docs.microsoft.com/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py#from-pandas-dataframe-dataframe--path-none--in-memory-false-) while creating a Dataset from in-memory DataFrame or local files that indeed DID cause errors in training **on remote compute**!

The imported classification algorithm was the scikit-learn module of Logistic-Regression [not Linear-Regression], which is a LINEAR Model for parameterizing the AutoMLConfig task of "classification" rather than "regression"!

HyperDrive used HyperDriveConfig to train a model imported from a traininig script; while the AutoML model was trained with hyperparameters tuning using an Azure ML Pipeline (with parameters to be listed in the next section).

Training script defined the clean_data() function (before cleaning & spitting), only then to define the main() function.

The hyperdrive config was used and it included both a parameter sampler, as well as an Early Stopping Policy (as its rationale of parameters choice to be explained below).

## Chosen Parameter Sampler Benefit & Rationale Explanation: 

The uniform random sampling (which much faster than the whole grid Sampling) returns search space's uniformly distributed random values.

The benefit of such chosen random parameter sampler was mainly explatory initial search that's speedy (with its support for early termination of low-performance runs); before further refining the search space to improve results of [discrete and continuous hyperparameters] feature engineering by retraining Hyperparameter Tuning with HyperDrive using Bayesian and/or 'Entire Grid' sampling, and AUC as the primary metric (rather than accuracy).

## Chosen Early Stopping Policy Benefit & Rationale Explanation

The benefit of the chosen Early Stopping [Bandit]Policy weas improving computational efficiency.

Explanation of the rationale for choosing a particular early stopping policy:
policy = BanditPolicy(evaluation_interval=2, slack_factor = 0.1)
The above Early Stopping Policy terminated poorly performing runs automatically, with [evaluation_interval] frequency of applying the policy every **other time** the train.py script logs the primary metric (AUC/Accuracy) rather than evertime when set to 1 by default; while **no** delay_evaluation minimum number of intervals was set (in order to avoid training runs premature termination; along with terminating any run that is not within the slack factor of 10% of (so far) best performing run.


# AutoML

## Description of the AutoML-generated Model and Hyperparameters:

Description of the model and hyperparameters generated by AutoML: VotingEnsemble (AutoML own model created using adjusted weights and hyper-parameters with a (XGBoostClassifier, LightGBM and SGD) composition of algorithms.

The created AutoMLConfig [for training] contained all the specifiable parameters (task, primary_metric, experiment_timeout_minutes, training_data, label_column_name, n_cross_validations).

The hyperparameters generated by the best AutoML model were:
```
[boosting: goss], [objective: binary], [metric: binary_logloss], [tree_learner: serial], [device_type: cpu], [num_iterations: 100], [learning_rate: 0.0421111], [num_leaves: 16], [max_depth: 4], [min_data_in_leaf: 1251], [min_sum_hessian_in_leaf: 6], [bagging_seed: 3], [feature_fraction: 0.594444], [feature_fraction_bynode: 1], [feature_fraction_seed: 2], [lambda_l1: 0.894737], [lambda_l2: 0.315789], [min_gain_to_split: 0.947368], [drop_rate: 0.1], [max_drop: 50], [skip_drop: 0.5], [drop_seed: 4], [top_rate: 0.2], [other_rate: 0.1], [min_data_per_group: 100], [max_cat_threshold: 32], [cat_l2: 10], [cat_smooth: 10], [max_cat_to_onehot: 4], [refit_decay_rate: 0.9], [verbosity: -10], [max_bin: 330], [min_data_in_bin: 3], [bin_construct_sample_cnt: 200000], [output_model: LightGBM_model.txt], [output_result: LightGBM_predict_result.txt], [is_enable_sparse: 1], [sparse_threshold: 0.8], [num_iteration_predict: -1], [pred_early_stop: 0], [pred_early_stop_freq: 10], [pred_early_stop_margin: 10], [convert_model: gbdt_prediction.cpp], [reg_sqrt: 0], [alpha: 0.9], [fair_c: 1], [poisson_max_delta_step: 0.7], [tweedie_variance_power: 1.5], [max_position: 20], [local_listen_port: 12400], [time_out: 120], , [gpu_platform_id: -1], [gpu_device_id: -1], [gpu_use_dp: 0].
```

# Pipeline Comparison

## comparing the two models performance (differences in accuracy):

The more accurate AutoML (with its deployabl models) outperformed the more [training] flexible HyperDrive hyperparameter tuning.

The AutoML algorithm of VotingEnsemble [that scored a %91.69 overall accuracy] outperformed the Logistic Regression model's accuracy of 0.9125948406676783 and with Regularization Strength = 0.5076209816219148; noting that the AutoML's pipeline steps had l1_ratio=0.3877551020408163, and fitted below regularised weights:
```
                 weights=[0.25, 0.25, 0.125,
                 0.125, 0.125,
                 0.125]
```

# Future Work

## Areas of Improvement Explanation:

Potential improvements (of the modles future experiments) could be identified as:

1.  Retraining Hyperparameter Tuning with HyperDrive using Bayesian and/or 'Entire Grid' sampling, and AUC as the primary metric (rather than accuracy).
2.  Converging a (>100) higher iterations maximum for the train script Main() function.
3.  Extending AutoML config to include more parameters.
4.  Considering RNN and/or DNN frameworks (for retraining an SOTA model) in the config of AutoML and/or the train script.
5.  Including top-level diagram of architecture or more diagrams of the overall pipeline and other ecosystem's microservices.
6.  Enhancing TabularDatasets API use and/or considering FileDatasetdownloading public URLs to be mounted to a compute datastore.
7.  Documenting XAI and [operationalizing] model [explanation] as a web service, using the [interpretability package] to explain ML models & predictions;
8.  Getting the code to check 1st [before creation attempt] for existing compute targets [pending].
9.  Exporting a Model to be run in Cloud Shell.
10.  Exporting Models with ONNX to be deployed to a DLT connected Dapp device (e.g. iOS or Android mobile device).
11.  Deploying web service by converting the trained model into a RESTful service that can be consumed by the other ecosystem microservices (including azure automation and/or logic apps) for optimal 'Decision Intelligence MLSecOps' synergy.
12. Enabling App Insights and integration with Sentinel and/or CASB.

The justification reasons that such improvement ways could help the model is because they ought to improve the model outcome (such as to boost the performance, to generalize better, to avoid overfitting and/or outliers).


# Proof of Cluster Clean Up

## Deleting he AMLCompute cluster_target

Following concluding the fitted training, function to remove the AMLCompute object [cluster_target.delete()] was duly actioned as in the below code:

```
from azureml.core.compute import ComputeTarget, AmlCompute # !1st
if delete_completion == True:
    try:
        # compute_target.delete()
        # aml_compute.delete() # compute_cluster
        cluster_target.delete()
        print('\n Attemting to delete the ComputeTarget (if found)...')
        # compute_target.wait_for_completion(show_output=False)  # verbose logging # True # !##
        # print(compute_target.status.serialize())
        # compute_target.wait_for_completion()
        print(cluster_target.status.serialize())
        cluster_target.wait_for_completion()
    except Exception as e:
        if '\n ComputeTarget Not Found!' in e.message:
            print("\n ComputeTarget has been deleted")
            delete_completion = False
```

## Image of Cluster Marked for Deletion:

Alternatively, enclosed is an image of the cluster [being selected or marked for deletion]:

![](images/image-of-cluster-marked-for-deletion.png?raw=true)
